{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language\n",
        " Processing (NLP) - Text\n",
        " Classification:"
      ],
      "metadata": {
        "id": "-gUmfeKI9VIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description: Classify text data into categories (e.g.,\n",
        " spam vs. non-spam, sentiment analysis)"
      ],
      "metadata": {
        "id": "GkTgQTPM9lL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6pDRqD629Gc",
        "outputId": "6c19e68d-9393-46b2-b1e6-2efa1da6efb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Initial Sentiment Analysis Data"
      ],
      "metadata": {
        "id": "NkOdVHz83t4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data files (will only run if they are not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 1. Data Loading and Preprocessing\n",
        "# Load the dataset from your file\n",
        "try:\n",
        "    df = pd.read_csv('/content/3) Sentiment dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'sentiment_data.csv' not found. Please check the file name and path.\")\n",
        "    exit()\n",
        "\n",
        "# Define the columns to use based on your image\n",
        "text_column = 'Text'\n",
        "label_column = 'Sentiment'\n",
        "\n",
        "# Drop the 'Unnamed: 0' column if it exists, as it's an index\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    df = df.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "# Preprocessing functions\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "df['processed_text'] = df[text_column].apply(preprocess_text)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['processed_text'],\n",
        "    df[label_column],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2. Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# 3. Model Training\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 4. Evaluation\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# --- FIXED: Added the zero_division parameter to remove the warning ---\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9qhzxEC7L2F",
        "outputId": "a47c96a9-32dc-4e48-f288-2aba2c916281"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.10884353741496598\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "         Acceptance          1.00      0.00      0.00         2\n",
            "           Admiration        1.00      0.00      0.00         1\n",
            "        Admiration           1.00      0.00      0.00         1\n",
            "         Affection           1.00      0.00      0.00         1\n",
            "      Ambivalence            1.00      0.00      0.00         1\n",
            "         Anger               1.00      0.00      0.00         1\n",
            "        Anticipation         1.00      0.00      0.00         1\n",
            "        Arousal              1.00      0.00      0.00         3\n",
            "                  Awe        1.00      0.00      0.00         1\n",
            "         Awe                 1.00      0.00      0.00         1\n",
            "                  Bad        1.00      0.00      0.00         1\n",
            "             Betrayal        1.00      0.00      0.00         2\n",
            "        Betrayal             1.00      0.00      0.00         1\n",
            "         Bitter              1.00      0.00      0.00         1\n",
            "           Bitterness        1.00      0.00      0.00         1\n",
            "          Bittersweet        1.00      0.00      0.00         1\n",
            "              Boredom        1.00      0.00      0.00         1\n",
            "         Calmness            1.00      0.00      0.00         1\n",
            "          Captivation        1.00      0.00      0.00         1\n",
            "     Celestial Wonder        1.00      0.00      0.00         1\n",
            "             Colorful        1.00      0.00      0.00         1\n",
            "      Confusion              1.00      0.00      0.00         3\n",
            "           Connection        1.00      0.00      0.00         1\n",
            "        Contemplation        1.00      0.00      0.00         1\n",
            "          Contentment        1.00      0.00      0.00         3\n",
            "        Contentment          1.00      0.00      0.00         1\n",
            "         Coziness            1.00      0.00      0.00         1\n",
            "         Creativity          1.00      0.00      0.00         1\n",
            "            Curiosity        1.00      0.00      0.00         2\n",
            "          Curiosity          1.00      0.00      0.00         1\n",
            "      Curiosity              1.00      0.00      0.00         2\n",
            "           Desolation        1.00      0.00      0.00         1\n",
            "           Devastated        1.00      0.00      0.00         2\n",
            "              Disgust        1.00      0.00      0.00         1\n",
            "         Disgust             1.00      0.00      0.00         2\n",
            "        Elation              1.00      0.00      0.00         3\n",
            "             Elegance        1.00      0.00      0.00         1\n",
            "          Embarrassed        1.00      0.00      0.00         1\n",
            "       EmotionalStorm        1.00      0.00      0.00         1\n",
            "        Empowerment          1.00      0.00      0.00         1\n",
            "         Enjoyment           1.00      0.00      0.00         2\n",
            "           Enthusiasm        1.00      0.00      0.00         1\n",
            "              Envious        1.00      0.00      0.00         2\n",
            "  Envisioning History        1.00      0.00      0.00         1\n",
            "         Euphoria            1.00      0.00      0.00         1\n",
            "           Excitement        0.08      0.33      0.12         3\n",
            "         Excitement          1.00      0.00      0.00         3\n",
            "        Excitement           1.00      0.00      0.00         1\n",
            "         Fear                1.00      0.00      0.00         1\n",
            "              Fearful        1.00      0.00      0.00         1\n",
            "           Frustrated        1.00      0.00      0.00         1\n",
            "          Frustration        1.00      0.00      0.00         3\n",
            "         Fulfillment         1.00      0.00      0.00         2\n",
            "             Grateful        1.00      0.00      0.00         1\n",
            "      Grief                  1.00      0.00      0.00         1\n",
            "                Happy        1.00      0.00      0.00         6\n",
            "                 Hate        1.00      0.00      0.00         2\n",
            "           Heartbreak        1.00      0.00      0.00         2\n",
            "              Hopeful        1.00      1.00      1.00         1\n",
            "        InnerJourney         1.00      0.00      0.00         1\n",
            "        Inspiration          1.00      0.00      0.00         1\n",
            "             Inspired        1.00      0.00      0.00         1\n",
            "            Isolation        1.00      0.00      0.00         1\n",
            "          Jealousy           1.00      0.00      0.00         1\n",
            "                  Joy        0.15      0.88      0.25         8\n",
            "         Joy                 1.00      0.00      0.00         1\n",
            "        JoyfulReunion        1.00      0.00      0.00         1\n",
            "         Kind                1.00      0.00      0.00         1\n",
            "           Loneliness        1.00      0.00      0.00         1\n",
            "      Loneliness             1.00      0.00      0.00         1\n",
            "             LostLove        1.00      0.00      0.00         1\n",
            "      Melancholy             1.00      0.00      0.00         2\n",
            "       Miscalculation        1.00      0.00      0.00         1\n",
            "              Neutral        1.00      0.00      0.00         1\n",
            "        Nostalgia            1.00      0.00      0.00         1\n",
            "      Nostalgia              1.00      0.00      0.00         1\n",
            "      Numbness               1.00      0.00      0.00         1\n",
            "          Overwhelmed        1.00      0.00      0.00         1\n",
            "              Playful        1.00      0.00      0.00         2\n",
            "            Positive         0.08      0.78      0.15         9\n",
            "                Proud        1.00      0.00      0.00         1\n",
            "        Reflection           1.00      0.00      0.00         1\n",
            "       Regret                1.00      0.00      0.00         1\n",
            "           Resilience        1.00      0.00      0.00         1\n",
            "            Reverence        1.00      0.00      0.00         1\n",
            "         Sadness             1.00      0.00      0.00         2\n",
            "        Satisfaction         1.00      0.00      0.00         1\n",
            "             Serenity        1.00      0.00      0.00         2\n",
            "      Serenity               1.00      0.00      0.00         2\n",
            "             Solitude        1.00      0.00      0.00         1\n",
            "          Sorrow             1.00      0.00      0.00         1\n",
            "         Spark               1.00      0.00      0.00         1\n",
            "         Surprise            1.00      0.00      0.00         1\n",
            "        Thrill               1.00      0.00      0.00         1\n",
            "             Vibrancy        1.00      0.00      0.00         1\n",
            " Whispers of the Past        1.00      0.00      0.00         1\n",
            "                 Zest        1.00      0.00      0.00         1\n",
            "\n",
            "              accuracy                           0.11       147\n",
            "             macro avg       0.97      0.03      0.02       147\n",
            "          weighted avg       0.88      0.11      0.03       147\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Email Phishing Dataset"
      ],
      "metadata": {
        "id": "uB5xxf_n3WIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87_ZRi6tvtZ3",
        "outputId": "4d9126b0-d6d7-429f-f022-c9a5b107d1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9897685052872249\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99    103573\n",
            "           1       0.85      0.28      0.42      1397\n",
            "\n",
            "    accuracy                           0.99    104970\n",
            "   macro avg       0.92      0.64      0.71    104970\n",
            "weighted avg       0.99      0.99      0.99    104970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier # A good choice for this type of data\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 1. Data Loading\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/email_phishing_data.csv')\n",
        "\n",
        "# 2. Data Preparation\n",
        "# Based on your image, the 'label' column is the target (what we want to predict).\n",
        "# All other columns are the features (what we will use to predict).\n",
        "X = df.drop('label', axis=1) # Features are all columns EXCEPT 'label'\n",
        "y = df['label']              # Target is the 'label' column\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3. Model Training\n",
        "# We'll use a RandomForestClassifier, which is effective for this type of data.\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Evaluation\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    }
  ]
}